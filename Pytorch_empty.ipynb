{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch is the premiere package for object-based machine-learning. It is currently used as the backend training for AI state-of-the-art technologies. E.g: Tesla's autopilot for self-driving cars and Stable Diffusion.\n",
    "### It is derived from the Lua-based torch library. We will expand on the use of PyTorch for constructing neural network models for simple applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tensor Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The default Torch array data structure is the Torch tensor. This class is almost completely analogous to its NumPy counterpart (the NumPy array), and bears a lot of the same methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array operations work similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.e: shape of array **must** match. This means each dimension must be equal, or any n-dimension may be matched with a dimension of size 1. E.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch tensors start to differ from NumPy on their seamless integration with automatic differentiation. The keyword *requires_grad* decides whether we record gradient operations on the declared tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$y =\\sum_i x_i^2$$\n",
    "\n",
    "### $$\\Rightarrow \\frac{\\partial y}{\\partial x_i} = 2x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically computing gradients is convenient for ML models, including those trained through gradient descent. In this case, automatic differentiation will be used in the context of $y = f(x)$ being our \"error\" or loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the tensor object poses some benefits in time complexity for (some) operations on large arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "N = 1000\n",
    "X = torch.randn((N,N))\n",
    "Y = torch.randn((N,N))\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "torch.matmul(X,Y)\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(\"Time is {}s\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "X1 = X.detach().numpy() #Converts to numpy array.\n",
    "Y1 = Y.detach().numpy()\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "X1@Y1\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print(\"Time is {}s\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without loss of generality, the goal of a neural network is to construct a model that predicts the behavior of a function $f$ that connects input x data, and output y data. For example, we want a general model to predict temperatures ($y$) based on the hour (t), the humidity (h), and the month (m). Thus, we let $\\hat{y}$ = f(t,h,m; $\\alpha$). In this case, $\\hat{y}$ is our predicted temperature for the parameters, while $y$ is the actual temperature. $\\alpha$ represents parameters we can tune through gradient descent.\n",
    "\n",
    "### Thus, we define a *Loss* function L($y$,$\\hat{y}$), which is analogous to our error. E.g: L($y$,$\\hat{y}$) = $\\sum_i (y_i-\\hat{y}_i)^2$ (mean square error). Minimizing this function by changing the $\\alpha$ parameters minimizes the \"error,\" and so, leads to a more accurate interpolation function $f$ (atleast for the inputs provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define two linear transformations $M_1, M_2$ that corresponds to the $\\alpha$ parameters mentioned prior (more on this later). We know our input is a 4x2 matrix, and we want a vector that is 4x1 as output. So if we multiply x by a 2xN, and then by a Nx1 matrix, our output will be a 4x1 matrix. The nn.linear function creates a function that maps an array to that array times a linear matrix multiplication (with a bias vector b). I.e: $g(x) = Ax+b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a class representing the aforementioned neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define the loss by the aforementioned mean-square error using nn.MSELoss():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "### Gradient descent is a relatively simple minimization problem. In this case, we minimize the error/loss by finding the $\\alpha$ parameters that lead to the error closest to 0. In 1-dimension (for $\\alpha$), this is done by finding $\\alpha$ such that $\\partial f/\\partial \\alpha = 0$. In this case, we adjust each of our parameters $\\alpha_i$ iteratively: \n",
    "\n",
    "### $$\\alpha_i \\rightarrow \\alpha_i-l\\frac{\\partial L}{\\partial a_i} ,$$\n",
    "### for n iterations (epochs) until we find a minimum in our loss function. Here, $l$ is the learning rate of your algorithm. The stochastic gradient descent inherent to PyTorch performs this process using automatic differentiation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_num)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)\n",
    "print(f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)\n",
    "print(f(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the following case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.plot(losses1)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "print(np.min(losses1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y1)\n",
    "print(g(x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this function (which is likely not linear), layers that are linear transformations aren't great approximations. As such, we add *activation layers* to our model. These are typically nonlinearities that add independence between our parameters represented by the aforementioned matrices. Consider our previous model: $$M_2(M_1x+b_1)+b_2 = M_2M_1x \\ \\ (b=\\hat{0}).$$\n",
    "\n",
    "### We now add a nonlinearity N (a **slightly** nonlinear function applied element-wise) right after our first linear transformation by $M_1$. Thus, our new model is given by:  $$M_2N(M_1x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of common nonlinearities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(-np.pi,np.pi, 1000)\n",
    "act = nn.ReLU()\n",
    "y = act(t)\n",
    "\n",
    "plt.plot(t.numpy(), (y).numpy())\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(-np.pi,np.pi, 1000)\n",
    "act = nn.Tanh()\n",
    "y = act(t)\n",
    "\n",
    "plt.plot(t.numpy(), (y).numpy())\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.linspace(-np.pi,np.pi, 1000)\n",
    "act = nn.Sigmoid()\n",
    "y = act(t)\n",
    "\n",
    "plt.plot(t.numpy(), (y).numpy())\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossNL)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No significant difference yet... but the addition of more parameters leads to significant corrections now that we have this nonlinearity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossNL)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y1)\n",
    "print(fNL(x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can add bias vectors to our layers so that our model is given by: \n",
    "\n",
    "### $$M_2(M_1x+b_1)+b_2.$$\n",
    "### This helps shift our results towards the positive or negative side depending on layers and inputs/outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossNLbias)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y1)\n",
    "print(fNLbias(x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we add another layer to our sequential neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossNLbiasF)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y1)\n",
    "print(fNLbiasF(x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting our output almost exactly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Integer Image Classification\n",
    "\n",
    "Reference: https://www.youtube.com/watch?v=gBw0u_5u0qU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the problem: We want to take an N x N image of a number from 0 to 9. This image might be hand-drawn, or computer-generated, but the training data should represent either or both of these samples. Our output should be a prediction of the number depicted based on the input image (either a scalar output, or the probabilities of each number being depicted in the given image). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    plt.imshow(x0[i].numpy())\n",
    "    plt.title(str(y0[i].numpy()))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use the one hot encoder to construct an array of values analogous to the probability of each class. In this case, our classes are 0-9 (10 classes total). Given an input image, we know the exact number related to that image. Thus, we set the probability of that given image being that particular number as 1. E.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch does NOT like working with 3-D arrays. As such, we flatten the images for our x, so that instead of having an 50000 x 28 x28 array, we have a 50000 x $28^2$ array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define our neural net and our loss function. In particular, our loss is now the cross-entropy loss. This loss function is better suited to classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossC)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "print(min(lossC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0])\n",
    "plt.title(torch.argmax(y_train[0]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the algorithm's predictions on training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10,4,figsize=(20,30)) \n",
    "for i in range(1,41):\n",
    "    plt.subplot(10,4,i)\n",
    "    plt.imshow(x_train[i-1])\n",
    "    plt.title(\"Model prediction is: {}\".format(torch.argmax(y_train[i-1]).numpy()))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10,4,figsize=(20,30)) \n",
    "for i in range(59001,59041):\n",
    "    plt.subplot(10,4,i-59000)\n",
    "    plt.imshow(x_train[i-1])\n",
    "    plt.title(\"Model prediction is: {}\".format(torch.argmax(y_train[i-1]).numpy()))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
